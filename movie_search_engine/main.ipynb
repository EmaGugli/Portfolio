{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lxml.html\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "import string\n",
    "from nltk.stem import PorterStemmer \n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "import io\n",
    "import time\n",
    "\n",
    "# Some variables that we are going to use throughout the code\n",
    "d = 20000    #number of documents to analyse\n",
    "ps = PorterStemmer()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Data collection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to get the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_u=[]\n",
    "soup = BeautifulSoup(response.text, \"lxml\")\n",
    "for link in soup.find_all('a'):\n",
    "    list_u.append(link.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'E:/DataScience/ADM/Data/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we download the files and save them as url files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=20000\n",
    "for link in list_u:\n",
    "        response1 = requests.get(link)\n",
    "        with io.open(''.join([path, 'article_', str(i), '.html']), 'w', encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                txt=response1.text\n",
    "            except:\n",
    "                time.sleep(2*60)\n",
    "                txt=response1.text\n",
    "            f.write(txt)\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we read the url files to extract the info we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collector():\n",
    "    for i in range(0,20000):\n",
    "        filename='E:/DataScience/ADM/Data/article_'+str(i)+'.html'\n",
    "        with io.open(filename,'r', encoding=\"utf-8\") as f:\n",
    "            s1=f.read()\n",
    "            soup = BeautifulSoup(s1, 'html')\n",
    "#extract the title of the page\n",
    "            try:\n",
    "                t=soup.title.text.strip('- Wikipedia')\n",
    "            except:\n",
    "                t=\"NA\"\n",
    "#find the \"h2\" , \"ul\" tags to find the intro and plot\n",
    "            intro=\"\"\n",
    "            try:\n",
    "                start1 = soup.find('table')\n",
    "                for elem in start1.next_siblings:\n",
    "                    if elem.name == 'h2':\n",
    "                    break\n",
    "                if elem.name != 'p':\n",
    "                    continue\n",
    "                intro1=elem.text        \n",
    "                intro=intro+intro1\n",
    "                intro=intro.strip('\\n')\n",
    "        except:\n",
    "            intro=\"NA\"\n",
    "        plot=\"\"    \n",
    "        try:\n",
    "            start2 = soup.find('div',id='toc')\n",
    "            \n",
    "            for el in start2.next_siblings:\n",
    "                if el.name == 'ul':\n",
    "                    break\n",
    "                if el.name == 'p':\n",
    "                    plot1=el.text\n",
    "                    plot=plot+plot1\n",
    "                    plot=plot.strip(\"\\n\")  \n",
    "        except:\n",
    "            plot='NA'\n",
    "#find the infobox by its class\n",
    "        tab=soup.find('table',class_='infobox vevent')\n",
    "        director = ''\n",
    "        producer=''\n",
    "        writer=''\n",
    "        starring=''\n",
    "        music=''\n",
    "        release_date=''\n",
    "        runtime=''\n",
    "        country=''\n",
    "        language=''\n",
    "        budget=''\n",
    "#try to find info we need form info box by the related tags\n",
    "        dic={}\n",
    "        try:\n",
    "            for r in tab.find_all('tr'):\n",
    "                td = r.find(\"td\")\n",
    "                if td != None:\n",
    "                    s = td\n",
    "                    p = r.find(\"th\")\n",
    "                    if p!=None:\n",
    "                        dic[p.text.strip(\"\\n\")] = s.text.strip(\"\\n\")\n",
    "            try:\n",
    "                director = dic['Directed by']\n",
    "            except:\n",
    "                dic['Directed by']=\"NA\"\n",
    "            try:\n",
    "                producer = dic['Produced by']\n",
    "            except:\n",
    "                dic['Produced by']=\"NA\"\n",
    "            try:\n",
    "                writer=dic['Written by']\n",
    "            except:\n",
    "                dic['Written by']=\"NA\"\n",
    "            try:\n",
    "                starring=dic['Starring']\n",
    "            except:\n",
    "                dic['Starring']=\"NA\"\n",
    "            try:\n",
    "                music=dic['Music by']\n",
    "            except:\n",
    "                dic['Music by']=\"NA\"\n",
    "            try:\n",
    "                release_date=dic['Release date']\n",
    "            except:\n",
    "                dic['Release date']=\"NA\"\n",
    "            try:\n",
    "                runtime=dic['Running time']\n",
    "            except:\n",
    "                dic['Running time']=\"NA\"\n",
    "            try:\n",
    "                country=dic['Country']\n",
    "            except:\n",
    "                dic['Country']=\"NA\"\n",
    "            try:\n",
    "                language=dic['Language']\n",
    "            except:\n",
    "                dic['Language']=\"NA\"\n",
    "            try:\n",
    "                budget=dic['Budget']\n",
    "            except:\n",
    "                dic[\"Budget\"] = \"NA\"\n",
    "#append all the information of a document to a list to create the tsv files\n",
    "            list_t=[]\n",
    "            list_t.append(t)\n",
    "            list_t.append(intro)\n",
    "            list_t.append(plot)\n",
    "            list_t.append(director)\n",
    "            list_t.append(producer)\n",
    "            list_t.append(writer)\n",
    "            list_t.append(starring)\n",
    "            list_t.append(music)\n",
    "            list_t.append(release_date)\n",
    "            list_t.append(runtime)\n",
    "            list_t.append(country)\n",
    "            list_t.append(language)\n",
    "            list_t.append(budget)\n",
    "#write the tsv file of each page\n",
    "            tsv_file= 'article_'+str(i)+'.tsv'\n",
    "            with open(tsv_file,'wt') as write_tsv:\n",
    "                tsv_writer = csv.writer(write_tsv, delimiter='\\t')\n",
    "                tsv_writer.writerow(list_t)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "                     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1) Preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invdict = defaultdict(list)\n",
    "voc = {}\n",
    "count = 0\n",
    "for j in range(d):\n",
    "    #handle error if file is not present\n",
    "    try:\n",
    "        tsvfile = open(r\"C:\\Users\\39335\\Desktop\\GitHub\\HW3\\tsv\\article_\"+str(j)+\".tsv\", \"r\")\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "    reader = tsvfile.readlines()\n",
    "    text = []\n",
    "    \n",
    "    # Transform the paragraph in a list with all the words\n",
    "    for row in reader:\n",
    "        text += row.split()\n",
    "\n",
    "    \n",
    "    sw = set(stopwords.words('english'))\n",
    "    ps = PorterStemmer()    \n",
    "    # Preprocess the words\n",
    "    \n",
    "    for k in range(len(text)):\n",
    "        # Remove stop words\n",
    "        text[k] = text[k].lower()\n",
    "        if text[k] in sw:\n",
    "            text[k] = \"\"\n",
    "\n",
    "        # Remove punctuation\n",
    "        table = str.maketrans(dict.fromkeys(string.punctuation))\n",
    "        text[k] = text[k].translate(table)\n",
    "\n",
    "        # Stemming\n",
    "        text[k] = ps.stem(text[k])\n",
    "        \n",
    "    \n",
    "    # We create a vocabulary that pairs each word to an ID.\n",
    "    # We make a inverted index where the key is the ID of the word and the value is a list of the ID\n",
    "    # of the documents that word is present\n",
    "    \n",
    "    # Remove empty elements\n",
    "    text[:] = [x for x in text if x != \"\"]\n",
    "    \n",
    "    for word in text:\n",
    "        \n",
    "        # Create vocabulary\n",
    "        if not word in voc:\n",
    "            voc[word] = count\n",
    "            count += 1\n",
    "        \n",
    "        # Create inverted dictionary\n",
    "        if j in invdict[voc[word]]:\n",
    "            continue\n",
    "        invdict[voc[word]].append(j)\n",
    "        \n",
    "            \n",
    "    tsvfile.close()\n",
    "\n",
    "# I now save the results in txt files so I can use them to search for the query leater.\n",
    "with open(\"Vocabulary.txt\", \"w\") as f:\n",
    "    f.write(json.dumps(voc))\n",
    "with open(\"InvertedIndex.txt\", \"w\") as f:\n",
    "    f.write(json.dumps(invdict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2) Execute the query. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we need to load the vocabulary and inverted index we already build."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Open the file with the vocabulary and the inverted dictionary\n",
    "with open(r'C:\\Users\\39335\\Downloads\\Vocabulary.txt', 'r') as v:\n",
    "    vocab = json.load(v)\n",
    "with open(r'C:\\Users\\39335\\Downloads\\InvertedIndex.txt', 'r') as i:\n",
    "    inverted = json.load(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we find all the documents that have all the words in the query. The search engine will be a function that takes as input the query and outputs a dataframe with the information of the movies that contains all the words in the query. If we need the ID of the documents (for example in ex 2.2.2) the second argument should be ID=True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SearchEngine1(q, ID = False):\n",
    "    query = []\n",
    "    # We need to stem the word of the query\n",
    "    \n",
    "    for word in q:\n",
    "        query.append(ps.stem(word))\n",
    "\n",
    "    querydict = {}\n",
    "    for word in query:\n",
    "        if word in vocab:\n",
    "            querydict[word] = vocab[word]\n",
    "        else:\n",
    "            print(\"The word %s was not found\" % word)\n",
    "\n",
    "    l = []\n",
    "\n",
    "    for i in inverted:\n",
    "        for ele in query:\n",
    "            if i == str(querydict[ele]):\n",
    "                l.append(inverted[i])\n",
    "\n",
    "    # I want to find the intersection between the lists of documents ID that contain the query\n",
    "    # (the documents that contains all the words we want)\n",
    "    inter = set(l[0])\n",
    "    for i in range(1, len(l)):\n",
    "        inter = inter.intersection(set(l[i]))\n",
    "    if len(inter)==0:\n",
    "        print(\"No documents satisfy all the elements in the query.\")\n",
    "        sys.exit()\n",
    "\n",
    "    # Inter is a set with the ID of the documents we need, now we open the html for each and extract the info we need\n",
    "    # I want to make 3 lists with title, intro and url for each movie that satisfies the query and then make a dataframe\n",
    "    title = []\n",
    "    introduction = []\n",
    "    url = []\n",
    "\n",
    "    for doc in inter:\n",
    "        file = open(r'C:\\Users\\39335\\Desktop\\HW3\\Data\\ article_'+str(doc)+'.html', 'r', encoding=\"utf-8\")\n",
    "        s1=file.read()\n",
    "        soup = BeautifulSoup(s1, 'html')\n",
    "        try:\n",
    "            t=soup.title.text.strip('- Wikipedia')\n",
    "        except:\n",
    "            t=\"NA\"\n",
    "        title.append(t)\n",
    "\n",
    "        intro=\"\"\n",
    "        try:\n",
    "            start1 = soup.find('table')\n",
    "            for elem in start1.next_siblings:\n",
    "                if elem.name == 'h2':\n",
    "                    break\n",
    "                if elem.name != 'p':\n",
    "                    continue\n",
    "                intro1=elem.text        \n",
    "                intro=intro+intro1\n",
    "                intro=intro.strip('\\n')\n",
    "        except:\n",
    "            intro=\"NA\"\n",
    "        introduction.append(intro)\n",
    "        u = \"https://en.wikipedia.org/wiki/\"+t\n",
    "        u = u.replace(\" \", \"_\")\n",
    "        url.append(u)\n",
    "        \n",
    "\n",
    "    # Make the data frame\n",
    "    if ID == True:\n",
    "        result = np.transpose(pd.DataFrame(np.array([title, introduction, url, list(inter)])))\n",
    "        result.columns = [\"Title\",\"Intro\", \"Wikipedia Url\", \"Document ID\"]\n",
    "    else:\n",
    "        result = np.transpose(pd.DataFrame(np.array([title, introduction, url])))\n",
    "        result.columns = [\"Title\",\"Intro\", \"Wikipedia Url\"]        \n",
    "    return (result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) Conjunctive query & Ranking score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to sort the documents we already obtained by their similarity with the given query. We start by defining  the function that returns a list that contains the tfIdf of a given document for each term term of the query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the tfIdf s for all the terms in a document\n",
    "def tfIdf(terms, documentID):\n",
    "    tfidf = []\n",
    "    # Evaluate Term Frequency\n",
    "    tsvfile = open(r\"C:\\Users\\39335\\Desktop\\GitHub\\HW3\\tsv\\article_\"+str(documentID)+\".tsv\", \"r\")\n",
    "    reader = tsvfile.readlines()\n",
    "    text = []\n",
    "\n",
    "        # Transform the paragraph in a list with all the words\n",
    "    for row in reader:\n",
    "        text += row.split()\n",
    "\n",
    "    sw = set(stopwords.words('english'))   \n",
    "    # Preprocess the words\n",
    "\n",
    "    for k in range(len(text)):\n",
    "        # Remove stop words\n",
    "        text[k] = text[k].lower()\n",
    "        if text[k] in sw:\n",
    "            text[k] = \"\"\n",
    "\n",
    "        # Remove punctuation\n",
    "        table = str.maketrans(dict.fromkeys(string.punctuation))\n",
    "        text[k] = text[k].translate(table)\n",
    "\n",
    "        # Stemming\n",
    "        text[k] = ps.stem(text[k])\n",
    "\n",
    "    # Remove empty elements\n",
    "    text[:] = [x for x in text if x != \"\"]\n",
    "\n",
    "    for term in terms:\n",
    "        tf = (text.count(term) / float(len(text)))\n",
    "\n",
    "        # Evaluate Inverse Document Frequency\n",
    "        numDocumentsWithTerm = len(SearchEngine([term]))\n",
    "        if numDocumentsWithTerm > 0:\n",
    "            idf = 1.0 + math.log(d / numDocumentsWithTerm)\n",
    "        else:\n",
    "            idf = 1.0\n",
    "    \n",
    "        tfidf.append(tf * idf)\n",
    "    return tfidf\n",
    "\n",
    "\n",
    "# Returns a list with the tfIdf for each work in the query calculated in the query itself\n",
    "def queryTFIDF(q):\n",
    "    tfidf = []\n",
    "    for i in range(len(q)):\n",
    "        tfidf.append((1/len(q) * (1 + math.log(len(q)))))\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define a new Search Engine function. It will output the same dataframe as before but only with the top \"k\" documents with the highest cosine similarity with the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SearchEngine2(q):\n",
    "    for word in q:           # Stem the query\n",
    "        word = ps.stem(word)\n",
    "    df = SearchEngine1(q, ID = True)\n",
    "    sim = []\n",
    "\n",
    "    for ele in df[\"Document ID\"]:\n",
    "        qtfIdf = queryTFIDF(q)\n",
    "        doctfIdf = tfIdf(q, ele)\n",
    "        cosim = (np.dot(qtfIdf, doctfIdf)/(np.linalg.norm(qtfIdf)*np.linalg.norm(doctfIdf)))\n",
    "        sim.append(cosim)\n",
    "        \n",
    "    df.insert(3, \"Similarity\", sim, True) \n",
    "    df = df.round({\"Similarity\" : 3})\n",
    "    df.drop(columns=\"Document ID\", inplace=True)\n",
    "    k = 3  # How many documents we want to see\n",
    "    df = df.sort_values(by='Similarity', kind='heapsort', ascending=False)  # Use heap data structure as a sorting algorithm\n",
    "    return(df.head(k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Define a new score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define another Search Engine that will sort the documents that match the query in order of the budget it was spent to make it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SearchEngine3(q):\n",
    "    for word in q:           # Stem the query\n",
    "        word = ps.stem(word)\n",
    "    df = SearchEngine1(q, ID = True)\n",
    "    BoxOffice = []\n",
    "\n",
    "    for ele in df[\"Document ID\"]:\n",
    "        filename=r'C:\\Users\\39335\\Desktop\\HW3\\Data\\ article_'+str(ele)+'.html'\n",
    "        with io.open(filename,'r', encoding=\"utf-8\") as f:\n",
    "            s1=f.read()\n",
    "            soup = BeautifulSoup(s1, 'html')\n",
    "        tab=soup.find('table',class_='infobox vevent')\n",
    "        box = \"\"\n",
    "        boxdic = {}\n",
    "\n",
    "        for r in tab.find_all('tr'):\n",
    "            td1 = r.find(\"td\")\n",
    "            if td1 != None:\n",
    "                s1 = td1\n",
    "                p1 = r.find(\"th\")\n",
    "                if p1!=None:\n",
    "                    boxdic[p1.text.strip(\"\\n\")] = s1.text.strip(\"\\n\")\n",
    "        try:\n",
    "            box = boxdic['Budget']\n",
    "        except:\n",
    "            boxdic['Budget'] = \"NA\"\n",
    "\n",
    "        BoxOffice.append(box)\n",
    "    df.insert(3, \"Budget\", BoxOffice, True) \n",
    "    df.drop(columns=\"Document ID\", inplace=True)\n",
    "    k = 3  # How many documents we want to see\n",
    "    df = df.sort_values(by='Budget', kind='heapsort', ascending=False)\n",
    "    return(df.head(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What search engine do you want to use? (1-Standard, 2-Sorted bu cosine similarity, 3-Sorted by budget\n",
      "1\n",
      "Insert the query to search. (Insert the words separated by a space)\n",
      "mom dad\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Intro</th>\n",
       "      <th>Wikipedia Url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Going Steady (1958 film)</td>\n",
       "      <td>Going Steady is a 1958 American film about two...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Going_Steady_(19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>The Boy Who Cried Werewolf (1973 film)</td>\n",
       "      <td>The Boy Who Cried Werewolf is a 1973 Technicol...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Boy_Who_Crie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>llflower (film)</td>\n",
       "      <td>Wallflower is a 1948 American comedy film dire...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/llflower_(film)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Bob and Sally</td>\n",
       "      <td>Bob and Sally (1948) is a movie produced in th...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Bob_and_Sally</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Mom and D</td>\n",
       "      <td>Mom and Dad (known as  The Family Story in the...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Mom_and_D</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Title  \\\n",
       "0                Going Steady (1958 film)   \n",
       "1  The Boy Who Cried Werewolf (1973 film)   \n",
       "2                         llflower (film)   \n",
       "3                           Bob and Sally   \n",
       "4                               Mom and D   \n",
       "\n",
       "                                               Intro  \\\n",
       "0  Going Steady is a 1958 American film about two...   \n",
       "1  The Boy Who Cried Werewolf is a 1973 Technicol...   \n",
       "2  Wallflower is a 1948 American comedy film dire...   \n",
       "3  Bob and Sally (1948) is a movie produced in th...   \n",
       "4  Mom and Dad (known as  The Family Story in the...   \n",
       "\n",
       "                                       Wikipedia Url  \n",
       "0  https://en.wikipedia.org/wiki/Going_Steady_(19...  \n",
       "1  https://en.wikipedia.org/wiki/The_Boy_Who_Crie...  \n",
       "2      https://en.wikipedia.org/wiki/llflower_(film)  \n",
       "3        https://en.wikipedia.org/wiki/Bob_and_Sally  \n",
       "4            https://en.wikipedia.org/wiki/Mom_and_D  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"What search engine do you want to use? (1-Standard, 2-Sorted bu cosine similarity, 3-Sorted by budget\")\n",
    "search_engine = int(input())\n",
    "print(\"Insert the query to search. (Insert the words separated by a space)\")\n",
    "q = input().split()\n",
    "\n",
    "if search_engine == 1:\n",
    "    result = SearchEngine1(q)\n",
    "if search_engine == 2:\n",
    "    result = SearchEngine2(q)\n",
    "if search_engine == 3:\n",
    "    result = SearchEngine3(q)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
